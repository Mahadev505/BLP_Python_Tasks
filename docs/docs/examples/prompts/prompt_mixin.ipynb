{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d1eb11b1",
      "metadata": {
        "id": "d1eb11b1"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/prompts/prompt_mixin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a5a641c-d90c-4401-ad59-369d70babf5b",
      "metadata": {
        "id": "4a5a641c-d90c-4401-ad59-369d70babf5b"
      },
      "source": [
        "# Accessing/Customizing Prompts within Higher-Level Modules\n",
        "\n",
        "LlamaIndex contains a variety of higher-level modules (query engines, response synthesizers, retrievers, etc.), many of which make LLM calls + use prompt templates.\n",
        "\n",
        "This guide shows how you can 1) access the set of prompts for any module (including nested) with `get_prompts`, and 2) update these prompts easily with `update_prompts`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b34cf919",
      "metadata": {
        "id": "b34cf919"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install LlamaIndex 🦙."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d8c8cd3e",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8c8cd3e",
        "outputId": "f65ff29c-5a40-4e66-851c-91b874e8570f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.11.23-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-index-agent-openai<0.4.0,>=0.3.4 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.3.4-py3-none-any.whl.metadata (728 bytes)\n",
            "Collecting llama-index-cli<0.4.0,>=0.3.1 (from llama-index)\n",
            "  Downloading llama_index_cli-0.3.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.12.0,>=0.11.23 (from llama-index)\n",
            "  Downloading llama_index_core-0.11.23-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.4 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl.metadata (686 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.4.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting llama-index-llms-openai<0.3.0,>=0.2.10 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.2.16-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.2.3-py3-none-any.whl.metadata (729 bytes)\n",
            "Collecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\n",
            "Collecting llama-index-readers-file<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.3.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.54.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.23->llama-index) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (3.10.10)\n",
            "Collecting dataclasses-json (from llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.2.14)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (11.0.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.16.0)\n",
            "Collecting llama-cloud>=0.1.5 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.5-py3-none-any.whl.metadata (763 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.4.0,>=0.3.0->llama-index) (4.12.3)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.4.0,>=0.3.0->llama-index)\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.4.0,>=0.3.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index)\n",
            "  Downloading llama_parse-0.5.14-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.4.0,>=0.3.0->llama-index) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (0.14.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (0.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.23->llama-index) (3.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.2.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.23->llama-index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (0.2.0)\n",
            "Downloading llama_index-0.11.23-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_agent_openai-0.3.4-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.3.1-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_core-0.11.23-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.4.2-py3-none-any.whl (10 kB)\n",
            "Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.2.16-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.2.3-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.3.0-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_cloud-0.1.5-py3-none-any.whl (188 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.0/189.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.5.14-py3-none-any.whl (13 kB)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m901.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, tenacity, pypdf, mypy-extensions, marshmallow, typing-inspect, tiktoken, llama-cloud, dataclasses-json, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 llama-cloud-0.1.5 llama-index-0.11.23 llama-index-agent-openai-0.3.4 llama-index-cli-0.3.1 llama-index-core-0.11.23 llama-index-embeddings-openai-0.2.5 llama-index-indices-managed-llama-cloud-0.4.2 llama-index-legacy-0.9.48.post4 llama-index-llms-openai-0.2.16 llama-index-multi-modal-llms-openai-0.2.3 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.3.0 llama-index-readers-llama-parse-0.3.0 llama-parse-0.5.14 marshmallow-3.23.1 mypy-extensions-1.0.0 pypdf-5.1.0 striprtf-0.0.26 tenacity-8.5.0 tiktoken-0.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "487c01b6-60ea-457f-8cc9-2a448d401d49",
      "metadata": {
        "id": "487c01b6-60ea-457f-8cc9-2a448d401d49"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5a94148e-a2f9-44b3-919f-4d9811ae27c1",
      "metadata": {
        "id": "5a94148e-a2f9-44b3-919f-4d9811ae27c1"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "352384e6-0f31-4aa9-a351-4d3b278e2afd",
      "metadata": {
        "id": "352384e6-0f31-4aa9-a351-4d3b278e2afd"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    load_index_from_storage,\n",
        "    StorageContext,\n",
        ")\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6edb6b21-565e-4993-98ab-bad44a2259b9",
      "metadata": {
        "id": "6edb6b21-565e-4993-98ab-bad44a2259b9"
      },
      "source": [
        "## Setup: Load Data, Build Index, and Get Query Engine\n",
        "\n",
        "Here we build a vector index over a toy dataset (PG's essay), and access the query engine.\n",
        "\n",
        "The query engine is a simple RAG pipeline consisting of top-k retrieval + LLM synthesis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e299e93",
      "metadata": {
        "id": "0e299e93"
      },
      "source": [
        "Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3c036532",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c036532",
        "outputId": "4a0be348-a566-4bd9-9707-1a327b6d27b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-13 10:40:34--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: ‘data/paul_graham/paul_graham_essay.txt’\n",
            "\n",
            "\r          data/paul   0%[                    ]       0  --.-KB/s               \rdata/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-11-13 10:40:35 (5.18 MB/s) - ‘data/paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p 'data/paul_graham/'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c0f1a04a-7859-4d97-810d-abde72891d1c",
      "metadata": {
        "id": "c0f1a04a-7859-4d97-810d-abde72891d1c"
      },
      "outputs": [],
      "source": [
        "# load documents\n",
        "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b84c7e74-c084-40b0-b94b-d454e095e746",
      "metadata": {
        "id": "b84c7e74-c084-40b0-b94b-d454e095e746"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "75b8534b-eb16-47a7-9269-dfd4ebf97fd7",
      "metadata": {
        "id": "75b8534b-eb16-47a7-9269-dfd4ebf97fd7"
      },
      "outputs": [],
      "source": [
        "# set Logging to DEBUG for more detailed outputs\n",
        "query_engine = index.as_query_engine(response_mode=\"tree_summarize\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d192cc45-f90a-4c8d-b9e0-29f70272a47e",
      "metadata": {
        "id": "d192cc45-f90a-4c8d-b9e0-29f70272a47e"
      },
      "outputs": [],
      "source": [
        "# define prompt viewing function\n",
        "def display_prompt_dict(prompts_dict):\n",
        "    for k, p in prompts_dict.items():\n",
        "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
        "        display(Markdown(text_md))\n",
        "        print(p.get_template())\n",
        "        display(Markdown(\"<br><br>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50d48e1c-15ce-4eb3-9e3c-00e7e547bf2c",
      "metadata": {
        "id": "50d48e1c-15ce-4eb3-9e3c-00e7e547bf2c"
      },
      "source": [
        "## Accessing Prompts\n",
        "\n",
        "Here we get the prompts from the query engine. Note that *all* prompts are returned, including ones used in sub-modules in the query engine. This allows you to centralize a view of these prompts!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7f28b46d-6115-4095-b1a2-5603a8ee709e",
      "metadata": {
        "id": "7f28b46d-6115-4095-b1a2-5603a8ee709e"
      },
      "outputs": [],
      "source": [
        "prompts_dict = query_engine.get_prompts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1413a347-817c-4648-a4ee-1ea00dcbd319",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "1413a347-817c-4648-a4ee-1ea00dcbd319",
        "outputId": "e0903961-b076-495d-b2fa-3afffe8f7954"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Prompt Key**: response_synthesizer:summary_template<br>**Text:** <br>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context information from multiple sources is below.\n",
            "---------------------\n",
            "{context_str}\n",
            "---------------------\n",
            "Given the information from multiple sources and not prior knowledge, answer the query.\n",
            "Query: {query_str}\n",
            "Answer: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<br><br>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "display_prompt_dict(prompts_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a110a58-2f93-42c5-a1ec-89ecc3692f93",
      "metadata": {
        "id": "1a110a58-2f93-42c5-a1ec-89ecc3692f93"
      },
      "source": [
        "#### Checking `get_prompts` on Response Synthesizer\n",
        "\n",
        "You can also call `get_prompts` on the underlying response synthesizer, where you'll see the same list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a2d99264-0f53-4da3-aa76-ab88f1425dc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "a2d99264-0f53-4da3-aa76-ab88f1425dc1",
        "outputId": "c5b7b06e-4063-41d4-cd20-3a831304a1c3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'RetrieverQueryEngine' object has no attribute 'response_synthesizer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-0f62c3a653bb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprompts_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_synthesizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdisplay_prompt_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'RetrieverQueryEngine' object has no attribute 'response_synthesizer'"
          ]
        }
      ],
      "source": [
        "prompts_dict = query_engine.response_synthesizer.get_prompts()\n",
        "display_prompt_dict(prompts_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdb003e4-e58b-46dd-abae-fb09b0c891a2",
      "metadata": {
        "id": "cdb003e4-e58b-46dd-abae-fb09b0c891a2"
      },
      "source": [
        "#### Checking `get_prompts` with a different response synthesis strategy\n",
        "\n",
        "Here we try the default `compact` method.\n",
        "\n",
        "We'll see that the set of templates used are different; a QA template and a refine template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d523fb00-7106-4849-8ce9-4bbc13dd912c",
      "metadata": {
        "id": "d523fb00-7106-4849-8ce9-4bbc13dd912c"
      },
      "outputs": [],
      "source": [
        "# set Logging to DEBUG for more detailed outputs\n",
        "query_engine = index.as_query_engine(response_mode=\"compact\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27474aef-06c3-4684-8778-9b00ba4ae7ef",
      "metadata": {
        "id": "27474aef-06c3-4684-8778-9b00ba4ae7ef",
        "outputId": "7fdef9ee-e8ff-4518-c70e-1e085d7cc989"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Prompt Key**: response_synthesizer:text_qa_template<br>**Text:** <br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context information is below.\n",
            "---------------------\n",
            "{context_str}\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: {query_str}\n",
            "Answer: \n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Prompt Key**: response_synthesizer:refine_template<br>**Text:** <br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The original query is as follows: {query_str}\n",
            "We have provided an existing answer: {existing_answer}\n",
            "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
            "------------\n",
            "{context_msg}\n",
            "------------\n",
            "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
            "Refined Answer: \n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompts_dict = query_engine.get_prompts()\n",
        "display_prompt_dict(prompts_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b545710e-bb29-4dd4-885c-bd9222e67b26",
      "metadata": {
        "id": "b545710e-bb29-4dd4-885c-bd9222e67b26"
      },
      "source": [
        "#### Put into query engine, get response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4936c416-bdd8-48d4-95c3-760b5f2a2bc8",
      "metadata": {
        "id": "4936c416-bdd8-48d4-95c3-760b5f2a2bc8",
        "outputId": "06d36258-e171-48b7-9a10-eacfcc141899"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The author worked on writing and programming outside of school before college. They wrote short stories and tried writing programs on an IBM 1401 computer using an early version of Fortran. They later got a microcomputer and started programming on it, writing simple games and a word processor. They also mentioned their interest in philosophy and AI.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"What did the author do growing up?\")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71103bb5-fe3e-4373-8cf9-b786f64f0189",
      "metadata": {
        "id": "71103bb5-fe3e-4373-8cf9-b786f64f0189"
      },
      "source": [
        "## Customize the prompt\n",
        "\n",
        "You can also update/customize the prompts with the `update_prompts` function. Pass in arg values with the keys equal to the keys you see in the prompt dictionary.\n",
        "\n",
        "Here we'll change the summary prompt to use Shakespeare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1545df36-4bd5-40d3-b4c8-a01c89774262",
      "metadata": {
        "id": "1545df36-4bd5-40d3-b4c8-a01c89774262"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "# reset\n",
        "query_engine = index.as_query_engine(response_mode=\"tree_summarize\")\n",
        "\n",
        "# shakespeare!\n",
        "new_summary_tmpl_str = (\n",
        "    \"Context information is below.\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Given the context information and not prior knowledge, \"\n",
        "    \"answer the query in the style of a Shakespeare play.\\n\"\n",
        "    \"Query: {query_str}\\n\"\n",
        "    \"Answer: \"\n",
        ")\n",
        "new_summary_tmpl = PromptTemplate(new_summary_tmpl_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bd7687b-bdbf-454e-b3b3-4438b80934a4",
      "metadata": {
        "id": "3bd7687b-bdbf-454e-b3b3-4438b80934a4"
      },
      "outputs": [],
      "source": [
        "query_engine.update_prompts(\n",
        "    {\"response_synthesizer:summary_template\": new_summary_tmpl}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad924fca-5074-4a04-99bc-e5dc0102d131",
      "metadata": {
        "id": "ad924fca-5074-4a04-99bc-e5dc0102d131"
      },
      "outputs": [],
      "source": [
        "prompts_dict = query_engine.get_prompts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc9e39a0-6db1-419f-816e-c702a8e49e04",
      "metadata": {
        "id": "cc9e39a0-6db1-419f-816e-c702a8e49e04",
        "outputId": "2631f867-ee27-49b9-e562-1532b5bb14b5"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Prompt Key**: response_synthesizer:summary_template<br>**Text:** <br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context information is below.\n",
            "---------------------\n",
            "{context_str}\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query in the style of a Shakespeare play.\n",
            "Query: {query_str}\n",
            "Answer: \n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display_prompt_dict(prompts_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b195c32-1913-4d1a-b6cf-3f2635e8773a",
      "metadata": {
        "id": "0b195c32-1913-4d1a-b6cf-3f2635e8773a"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"What did the author do growing up?\")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "518c793c-e70a-4f69-b03d-adc0acd6fb2c",
      "metadata": {
        "id": "518c793c-e70a-4f69-b03d-adc0acd6fb2c"
      },
      "source": [
        "## Accessing Prompts from Other Modules\n",
        "\n",
        "Here we take a look at some other modules: query engines, routers/selectors, evaluators, and others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3bf5ee9-ef06-46e0-bcf4-6e1ad90e3f1a",
      "metadata": {
        "id": "f3bf5ee9-ef06-46e0-bcf4-6e1ad90e3f1a"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_engine import (\n",
        "    RouterQueryEngine,\n",
        "    FLAREInstructQueryEngine,\n",
        ")\n",
        "from llama_index.core.selectors import LLMMultiSelector\n",
        "from llama_index.core.evaluation import FaithfulnessEvaluator, DatasetGenerator\n",
        "from llama_index.core.postprocessor import LLMRerank"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00c69460-8361-4cef-ba67-af593f3755a1",
      "metadata": {
        "id": "00c69460-8361-4cef-ba67-af593f3755a1"
      },
      "source": [
        "#### Analyze Prompts: Router Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96090e36-314c-4758-b193-bc4608e4d3f9",
      "metadata": {
        "id": "96090e36-314c-4758-b193-bc4608e4d3f9"
      },
      "outputs": [],
      "source": [
        "# setup sample router query engine\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "query_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=query_engine, description=\"test description\"\n",
        ")\n",
        "\n",
        "router_query_engine = RouterQueryEngine.from_defaults([query_tool])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25195335-e641-4bb5-8e46-5a950a5e0674",
      "metadata": {
        "id": "25195335-e641-4bb5-8e46-5a950a5e0674",
        "outputId": "5dd59e43-791b-4490-9e02-54efc8a1673e"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Prompt Key**: summarizer:summary_template<br>**Text:** <br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context information from multiple sources is below.\n",
            "---------------------\n",
            "{context_str}\n",
            "---------------------\n",
            "Given the information from multiple sources and not prior knowledge, answer the query.\n",
            "Query: {query_str}\n",
            "Answer: \n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompts_dict = router_query_engine.get_prompts()\n",
        "display_prompt_dict(prompts_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3847ce81-8d1a-475d-b293-5b10458197bc",
      "metadata": {
        "id": "3847ce81-8d1a-475d-b293-5b10458197bc"
      },
      "source": [
        "#### Analyze Prompts: FLARE Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d555edd-6c5e-4c68-b06c-8e1e0c365a17",
      "metadata": {
        "id": "4d555edd-6c5e-4c68-b06c-8e1e0c365a17"
      },
      "outputs": [],
      "source": [
        "flare_query_engine = FLAREInstructQueryEngine(query_engine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc7d8f2d-a522-41af-bfbb-9842e03af595",
      "metadata": {
        "id": "dc7d8f2d-a522-41af-bfbb-9842e03af595",
        "outputId": "5b96709a-f96f-46e3-d66c-073fee2b1e33"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Prompt Key**: instruct_prompt<br>**Text:** <br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skill 1. Use the Search API to look up relevant information by writing     \"[Search(query)]\" where \"query\" is the search query you want to look up.     For example:\n",
            "\n",
            "Query: But what are the risks during production of nanomaterials?\n",
            "Answer: [Search(What are some nanomaterial production risks?)]\n",
            "\n",
            "Query: The colors on the flag of Ghana have the following meanings.\n",
            "Answer: Red is for [Search(What is the meaning of Ghana's flag being red?)],     green for forests, and gold for mineral wealth.\n",
            "\n",
            "Query: What did the author do during his time in college?\n",
            "Answer: The author took classes in [Search(What classes did the author take in     college?)].\n",
            "\n",
            "\n",
            "\n",
            "Skill 2. Solve more complex generation tasks by thinking step by step. For example:\n",
            "\n",
            "Query: Give a summary of the author's life and career.\n",
            "Answer: The author was born in 1990. Growing up, he [Search(What did the     author do during his childhood?)].\n",
            "\n",
            "Query: Can you write a summary of the Great Gatsby.\n",
            "Answer: The Great Gatsby is a novel written by F. Scott Fitzgerald. It is about     [Search(What is the Great Gatsby about?)].\n",
            "\n",
            "\n",
            "Now given the following task, and the stub of an existing answer, generate the next portion of the answer. You may use the Search API \"[Search(query)]\" whenever possible.\n",
            "If the answer is complete and no longer contains any \"[Search(query)]\" tags, write     \"done\" to finish the task.\n",
            "Do not write \"done\" if the answer still contains \"[Search(query)]\" tags.\n",
            "Do not make up answers. It is better to generate one \"[Search(query)]\" tag and stop generation\n",
            "than to fill in the answer with made up information with no \"[Search(query)]\" tags\n",
            "or multiple \"[Search(query)]\" tags that assume a structure in the answer.\n",
            "Try to limit generation to one sentence if possible.\n",
            "\n",
            "\n",
            "Query: {query_str}\n",
            "Existing Answer: {existing_answer}\n",
            "Answer: \n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Prompt Key**: query_engine:response_synthesizer:summary_template<br>**Text:** <br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context information is below.\n",
            "---------------------\n",
            "{context_str}\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query in the style of a Shakespeare play.\n",
            "Query: {query_str}\n",
            "Answer: \n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Prompt Key**: lookahead_answer_inserter:answer_insert_prompt<br>**Text:** <br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "An existing 'lookahead response' is given below. The lookahead response\n",
            "contains `[Search(query)]` tags. Some queries have been executed and the\n",
            "response retrieved. The queries and answers are also given below.\n",
            "Also the previous response (the response before the lookahead response)\n",
            "is given below.\n",
            "Given the lookahead template, previous response, and also queries and answers,\n",
            "please 'fill in' the lookahead template with the appropriate answers.\n",
            "\n",
            "NOTE: Please make sure that the final response grammatically follows\n",
            "the previous response + lookahead template. For example, if the previous\n",
            "response is \"New York City has a population of \" and the lookahead\n",
            "template is \"[Search(What is the population of New York City?)]\", then\n",
            "the final response should be \"8.4 million\".\n",
            "\n",
            "NOTE: the lookahead template may not be a complete sentence and may\n",
            "contain trailing/leading commas, etc. Please preserve the original\n",
            "formatting of the lookahead template if possible.\n",
            "\n",
            "NOTE:\n",
            "\n",
            "NOTE: the exception to the above rule is if the answer to a query\n",
            "is equivalent to \"I don't know\" or \"I don't have an answer\". In this case,\n",
            "modify the lookahead template to indicate that the answer is not known.\n",
            "\n",
            "NOTE: the lookahead template may contain multiple `[Search(query)]` tags\n",
            "    and only a subset of these queries have been executed.\n",
            "    Do not replace the `[Search(query)]` tags that have not been executed.\n",
            "\n",
            "Previous Response:\n",
            "\n",
            "\n",
            "Lookahead Template:\n",
            "Red is for [Search(What is the meaning of Ghana's     flag being red?)], green for forests, and gold for mineral wealth.\n",
            "\n",
            "Query-Answer Pairs:\n",
            "Query: What is the meaning of Ghana's flag being red?\n",
            "Answer: The red represents the blood of those who died in the country's struggle     for independence\n",
            "\n",
            "Filled in Answers:\n",
            "Red is for the blood of those who died in the country's struggle for independence,     green for forests, and gold for mineral wealth.\n",
            "\n",
            "Previous Response:\n",
            "One of the largest cities in the world\n",
            "\n",
            "Lookahead Template:\n",
            ", the city contains a population of [Search(What is the population     of New York City?)]\n",
            "\n",
            "Query-Answer Pairs:\n",
            "Query: What is the population of New York City?\n",
            "Answer: The population of New York City is 8.4 million\n",
            "\n",
            "Synthesized Response:\n",
            ", the city contains a population of 8.4 million\n",
            "\n",
            "Previous Response:\n",
            "the city contains a population of\n",
            "\n",
            "Lookahead Template:\n",
            "[Search(What is the population of New York City?)]\n",
            "\n",
            "Query-Answer Pairs:\n",
            "Query: What is the population of New York City?\n",
            "Answer: The population of New York City is 8.4 million\n",
            "\n",
            "Synthesized Response:\n",
            "8.4 million\n",
            "\n",
            "Previous Response:\n",
            "{prev_response}\n",
            "\n",
            "Lookahead Template:\n",
            "{lookahead_response}\n",
            "\n",
            "Query-Answer Pairs:\n",
            "{query_answer_pairs}\n",
            "\n",
            "Synthesized Response:\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompts_dict = flare_query_engine.get_prompts()\n",
        "display_prompt_dict(prompts_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "214a767a-3681-46b3-b073-9dc1744a1f9d",
      "metadata": {
        "id": "214a767a-3681-46b3-b073-9dc1744a1f9d"
      },
      "source": [
        "#### Analyze Prompts: LLMMultiSelector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8254af7-076d-4cc3-9a9f-d92686b8f307",
      "metadata": {
        "id": "d8254af7-076d-4cc3-9a9f-d92686b8f307"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "selector = LLMSingleSelector.from_defaults()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f3f0d7d-a19b-46cf-946f-6259c70c93df",
      "metadata": {
        "id": "3f3f0d7d-a19b-46cf-946f-6259c70c93df",
        "outputId": "c75aaee4-3eee-4ef5-a432-ba6ee5ad33da"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Prompt Key**: prompt<br>**Text:** <br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Some choices are given below. It is provided in a numbered list (1 to {num_choices}), where each item in the list corresponds to a summary.\n",
            "---------------------\n",
            "{context_list}\n",
            "---------------------\n",
            "Using only the choices above and not prior knowledge, return the choice that is most relevant to the question: '{query_str}'\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompts_dict = selector.get_prompts()\n",
        "display_prompt_dict(prompts_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12b47039-0b47-4de6-b408-f04d52bb66e3",
      "metadata": {
        "id": "12b47039-0b47-4de6-b408-f04d52bb66e3"
      },
      "source": [
        "#### Analyze Prompts: FaithfulnessEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4368750-6229-4f71-b194-deb0013ab5d7",
      "metadata": {
        "id": "a4368750-6229-4f71-b194-deb0013ab5d7"
      },
      "outputs": [],
      "source": [
        "evaluator = FaithfulnessEvaluator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac4ac1f0-a071-4bd0-95bb-54d7bce07e1b",
      "metadata": {
        "id": "ac4ac1f0-a071-4bd0-95bb-54d7bce07e1b",
        "outputId": "1c803850-0e90-4e36-e2f5-26cd7697d6dc"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Prompt Key**: eval_template<br>**Text:** <br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please tell if a given piece of information is supported by the context.\n",
            "You need to answer with either YES or NO.\n",
            "Answer YES if any of the context supports the information, even if most of the context is unrelated. Some examples are provided below. \n",
            "\n",
            "Information: Apple pie is generally double-crusted.\n",
            "Context: An apple pie is a fruit pie in which the principal filling ingredient is apples. \n",
            "Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard or cheddar cheese.\n",
            "It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
            "Answer: YES\n",
            "Information: Apple pies tastes bad.\n",
            "Context: An apple pie is a fruit pie in which the principal filling ingredient is apples. \n",
            "Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard or cheddar cheese.\n",
            "It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
            "Answer: NO\n",
            "Information: {query_str}\n",
            "Context: {context_str}\n",
            "Answer: \n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Prompt Key**: refine_template<br>**Text:** <br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We want to understand if the following information is present in the context information: {query_str}\n",
            "We have provided an existing YES/NO answer: {existing_answer}\n",
            "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
            "------------\n",
            "{context_msg}\n",
            "------------\n",
            "If the existing answer was already YES, still answer YES. If the information is present in the new context, answer YES. Otherwise answer NO.\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompts_dict = evaluator.get_prompts()\n",
        "display_prompt_dict(prompts_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62a8b47c-f827-4bb2-990a-661b1d729b7e",
      "metadata": {
        "id": "62a8b47c-f827-4bb2-990a-661b1d729b7e"
      },
      "source": [
        "#### Analyze Prompts: DatasetGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1edfc3d4-5c22-4d2c-aead-1fa35a04233a",
      "metadata": {
        "id": "1edfc3d4-5c22-4d2c-aead-1fa35a04233a"
      },
      "outputs": [],
      "source": [
        "dataset_generator = DatasetGenerator.from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19dc22df-893f-4553-829c-7e31a16ee9a4",
      "metadata": {
        "id": "19dc22df-893f-4553-829c-7e31a16ee9a4",
        "outputId": "1eaf1907-24dc-4377-a7e6-047f7bb548ca"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Prompt Key**: text_question_template<br>**Text:** <br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context information is below.\n",
            "---------------------\n",
            "{context_str}\n",
            "---------------------\n",
            "Given the context information and not prior knowledge.\n",
            "generate only questions based on the below query.\n",
            "{query_str}\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Prompt Key**: text_qa_template<br>**Text:** <br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context information is below.\n",
            "---------------------\n",
            "{context_str}\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: {query_str}\n",
            "Answer: \n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompts_dict = dataset_generator.get_prompts()\n",
        "display_prompt_dict(prompts_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fe09fc5-108c-40e6-8343-7e662bf2f67c",
      "metadata": {
        "id": "5fe09fc5-108c-40e6-8343-7e662bf2f67c"
      },
      "source": [
        "#### Analyze Prompts: LLMRerank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e3269cc-7767-46c1-a252-92de343f2d9b",
      "metadata": {
        "id": "5e3269cc-7767-46c1-a252-92de343f2d9b"
      },
      "outputs": [],
      "source": [
        "llm_rerank = LLMRerank()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94cbc425-7276-4973-8426-ea83c693c949",
      "metadata": {
        "id": "94cbc425-7276-4973-8426-ea83c693c949",
        "outputId": "100dc8d6-daa6-46da-8cb7-8142360fe961"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Prompt Key**: text_question_template<br>**Text:** <br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context information is below.\n",
            "---------------------\n",
            "{context_str}\n",
            "---------------------\n",
            "Given the context information and not prior knowledge.\n",
            "generate only questions based on the below query.\n",
            "{query_str}\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Prompt Key**: text_qa_template<br>**Text:** <br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context information is below.\n",
            "---------------------\n",
            "{context_str}\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: {query_str}\n",
            "Answer: \n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompts_dict = dataset_generator.get_prompts()\n",
        "display_prompt_dict(prompts_dict)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llama_index_v2",
      "language": "python",
      "name": "llama_index_v2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}